# Hafta 7: Ensemble Learning ve Model DeÄŸerlendirme

## ğŸ“‹ Ensemble Learning Nedir?

Birden fazla modeli birleÅŸtirerek daha iyi tahmin yapma.

**Motivasyon:** "Birlik gÃ¼Ã§tÃ¼r" - Ã‡ok sayÄ±da zayÄ±f model, bir gÃ¼Ã§lÃ¼ model oluÅŸturabilir.

## ğŸ¯ Ensemble TÃ¼rleri

### 1. Bagging (Bootstrap Aggregating)

**Ä°lke:** Paralel modeller, varyansÄ± azalt

**Bootstrap:** Yerine koyarak Ã¶rnekleme
- Orijinal veri: N Ã¶rnek
- Bootstrap sample: N Ã¶rnek (tekrarlÄ±)
- Her model farklÄ± bootstrap sample'da eÄŸitilir

**Voting:**
- Classification: Ã‡oÄŸunluk oylamasÄ±
- Regression: Ortalama

### 2. Boosting

**Ä°lke:** SÄ±ralÄ± modeller, bias'Ä± azalt

Her yeni model, Ã¶nceki modellerin hatalarÄ±na odaklanÄ±r.

**AdÄ±mlar:**
1. Ä°lk model eÄŸit
2. HatalÄ± Ã¶rneklere aÄŸÄ±rlÄ±k ver
3. Yeni model eÄŸit
4. Tekrarla

### 3. Stacking

FarklÄ± modellerin Ã§Ä±ktÄ±larÄ±nÄ± bir meta-model ile birleÅŸtir.

## ğŸŒ² Random Forest

**Karar aÄŸaÃ§larÄ±nÄ±n bagging'i + Ã¶znitelik rastgeleliÄŸi**

### Algoritma
```
For b = 1 to B:
    1. Bootstrap sample oluÅŸtur
    2. Her split'te rastgele m Ã¶znitelik seÃ§
    3. Karar aÄŸacÄ± oluÅŸtur (tam bÃ¼yÃ¼t, budama yok)
    
Tahmin = TÃ¼m aÄŸaÃ§larÄ±n ortalamasÄ±/Ã§oÄŸunluk oylamasÄ±
```

### Parametreler
- **n_estimators:** AÄŸaÃ§ sayÄ±sÄ± (100-500 iyi)
- **max_depth:** Maksimum derinlik
- **min_samples_split:** BÃ¶lme iÃ§in minimum Ã¶rnek
- **max_features:** Her split'te kullanÄ±lacak Ã¶znitelik sayÄ±sÄ±
  - "sqrt": âˆšn (classification default)
  - "log2": logâ‚‚(n)
  - None: TÃ¼m Ã¶znitelikler

### Avantajlar
âœ… YÃ¼ksek accuracy
âœ… Overfitting'e dayanÄ±klÄ±
âœ… Feature importance
âœ… Out-of-bag (OOB) error
âœ… Paralel eÄŸitilebilir

âŒ YorumlamasÄ± zor
âŒ BÃ¼yÃ¼k model boyutu
âŒ YavaÅŸ tahmin

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf.fit(X_train, y_train)

# Feature importance
importances = rf.feature_importances_
```

## ğŸš€ Gradient Boosting

### AdaBoost (Adaptive Boosting)

```
1. TÃ¼m Ã¶rneklere eÅŸit aÄŸÄ±rlÄ±k ver: w_i = 1/N
2. For m = 1 to M:
    a. AÄŸÄ±rlÄ±klÄ± veriyle model eÄŸit
    b. Hata hesapla
    c. Model aÄŸÄ±rlÄ±ÄŸÄ± hesapla: Î±_m
    d. YanlÄ±ÅŸ Ã¶rneklere daha fazla aÄŸÄ±rlÄ±k ver
3. Final = Î£(Î±_m Ã— model_m)
```

### Gradient Boosting Machine (GBM)

Gradient descent ile hatalarÄ± azaltma.

```
F_0(x) = baÅŸlangÄ±Ã§ tahmini
For m = 1 to M:
    1. Residual hesapla: r = y - F_(m-1)(x)
    2. Yeni model eÄŸit: h_m(x) â‰ˆ r
    3. GÃ¼ncelle: F_m(x) = F_(m-1)(x) + Î· Ã— h_m(x)
```

### XGBoost (Extreme Gradient Boosting)

**GeliÅŸmiÅŸ GBM implementasyonu**
- Regularization (L1, L2)
- Paralel iÅŸlem
- Tree pruning
- Missing value handling

```python
import xgboost as xgb

model = xgb.XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    random_state=42
)
model.fit(X_train, y_train)
```

## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma

| Algoritma | HÄ±z | Accuracy | Overfitting | Yorumlama |
|-----------|-----|----------|-------------|-----------|
| Tek AÄŸaÃ§ | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­â­â­ |
| Random Forest | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ |
| AdaBoost | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­ |
| XGBoost | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ |

## ğŸ¯ Model DeÄŸerlendirme - Derinlemesine

### Cross-Validation Stratejileri

**K-Fold CV:**
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
```

**Stratified K-Fold:** SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± korur
```python
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5)
for train_idx, test_idx in skf.split(X, y):
    # ...
```

**Time Series Split:** Zaman serisi iÃ§in
```python
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
```

### ROC Curve ve AUC

```python
from sklearn.metrics import roc_curve, auc

y_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
```

### Precision-Recall Curve

Dengesiz veri setleri iÃ§in ROC'tan daha bilgilendirici.

```python
from sklearn.metrics import precision_recall_curve

precision, recall, _ = precision_recall_curve(y_test, y_proba)
plt.plot(recall, precision)
```

### Learning Curve

Model ne kadar veri ile ne kadar Ã¶ÄŸreniyor?

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, test_scores = learning_curve(
    model, X, y, cv=5, 
    train_sizes=np.linspace(0.1, 1.0, 10)
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Train')
plt.plot(train_sizes, test_scores.mean(axis=1), label='Test')
plt.xlabel('Training Examples')
plt.ylabel('Score')
plt.legend()
```

## ğŸ”§ Hiperparametre Optimizasyonu

### Grid Search
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(
    RandomForestClassifier(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)
grid.fit(X_train, y_train)

print("Best params:", grid.best_params_)
print("Best score:", grid.best_score_)
```

### Randomized Search
Daha hÄ±zlÄ±, rastgele kombinasyonlar dener.

```python
from sklearn.model_selection import RandomizedSearchCV

random_search = RandomizedSearchCV(
    model, param_distributions=param_grid,
    n_iter=20, cv=5, random_state=42
)
```

## ğŸ’¡ Ä°puÃ§larÄ±

1. **BaÅŸla basit:** Ã–nce tek model, sonra ensemble
2. **Cross-validation:** Her zaman CV kullan
3. **Baseline:** Basit bir baseline model oluÅŸtur
4. **Feature engineering:** Model kadar Ã¶nemli
5. **Compute trade-off:** Accuracy vs hÄ±z dengesi
6. **Hyperparameter tuning:** Sonunda yap, Ã¶nce deÄŸil
