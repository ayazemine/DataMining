# Hafta 13: BÃ¼yÃ¼k Veri Teknolojileri

## ğŸ“‹ BÃ¼yÃ¼k Veri Nedir?

### 3V Modeli (Klasik)
- **Volume (Hacim):** TB, PB, EB boyutunda veri
- **Velocity (HÄ±z):** HÄ±zlÄ± veri akÄ±ÅŸÄ±
- **Variety (Ã‡eÅŸitlilik):** FarklÄ± formatlarda veri

### 5V Modeli (GeniÅŸletilmiÅŸ)
- **Veracity (DoÄŸruluk):** Veri kalitesi
- **Value (DeÄŸer):** Ä°ÅŸ deÄŸeri

## ğŸ˜ Hadoop Ekosistemi

### Hadoop Nedir?
DaÄŸÄ±tÄ±k veri depolama ve iÅŸleme framework'Ã¼.

### BileÅŸenler

**1. HDFS (Hadoop Distributed File System)**
- Verileri bloklar halinde daÄŸÄ±tÄ±k depolar
- VarsayÄ±lan blok boyutu: 128 MB
- Replication: Her blok 3 kopya (varsayÄ±lan)
- NameNode: Metadata yÃ¶netimi
- DataNode: Veri depolama

**2. YARN (Yet Another Resource Negotiator)**
- Kaynak yÃ¶netimi
- Ä°ÅŸ planlama
- ResourceManager + NodeManager

**3. MapReduce**
- DaÄŸÄ±tÄ±k veri iÅŸleme modeli
- Map: Paralel iÅŸlem
- Reduce: SonuÃ§larÄ± birleÅŸtir

## ğŸ—ºï¸ MapReduce Programlama Modeli

### Map Fonksiyonu
```
map(key1, value1) â†’ list(key2, value2)
```

### Reduce Fonksiyonu
```
reduce(key2, list(value2)) â†’ list(value3)
```

### Ã–rnek: Kelime Sayma (Word Count)

**Input:**
```
Hello World
Hello Hadoop
```

**Map Phase:**
```
(Hello, 1)
(World, 1)
(Hello, 1)
(Hadoop, 1)
```

**Shuffle & Sort:**
```
(Hadoop, [1])
(Hello, [1, 1])
(World, [1])
```

**Reduce Phase:**
```
(Hadoop, 1)
(Hello, 2)
(World, 1)
```

## âš¡ Apache Spark

### Spark vs Hadoop

| Ã–zellik | Hadoop MapReduce | Apache Spark |
|---------|------------------|--------------|
| HÄ±z | YavaÅŸ (disk I/O) | HÄ±zlÄ± (in-memory) |
| KullanÄ±m | KarmaÅŸÄ±k | Kolay (Python, Scala, Java, R) |
| ML/Graph | SÄ±nÄ±rlÄ± | MLlib, GraphX |
| Streaming | Zor | Spark Streaming |

### Spark RDD (Resilient Distributed Dataset)

DaÄŸÄ±tÄ±k, deÄŸiÅŸmez veri koleksiyonu.

**Transformations (Lazy):**
- map, filter, flatMap
- join, union
- groupByKey, reduceByKey

**Actions (Eager):**
- count, collect, save
- reduce, fold

### PySpark Ã–rnek

```python
from pyspark.sql import SparkSession

# Spark session oluÅŸtur
spark = SparkSession.builder \
    .appName("WordCount") \
    .getOrCreate()

# Veri oku
text = spark.read.text("input.txt")

# Word count
from pyspark.sql.functions import explode, split

words = text.select(explode(split(text.value, " ")).alias("word"))
word_count = words.groupBy("word").count()

# SonuÃ§larÄ± gÃ¶ster
word_count.show()
```

### Spark DataFrame

Pandas benzeri API, daÄŸÄ±tÄ±k.

```python
# CSV okuma
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# SQL sorgularÄ±
df.createOrReplaceTempView("people")
result = spark.sql("SELECT * FROM people WHERE age > 30")

# Transformations
df.select("name", "age") \
  .filter(df.age > 21) \
  .groupBy("age").count() \
  .show()
```

## ğŸ¤– Spark MLlib

DaÄŸÄ±tÄ±k makine Ã¶ÄŸrenmesi kÃ¼tÃ¼phanesi.

### Ã–rnek: Classification

```python
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import VectorAssembler

# Feature vector oluÅŸtur
assembler = VectorAssembler(
    inputCols=["feature1", "feature2", "feature3"],
    outputCol="features"
)
df_features = assembler.transform(df)

# Train-test split
train, test = df_features.randomSplit([0.7, 0.3])

# Model eÄŸit
lr = LogisticRegression(featuresCol="features", labelCol="label")
model = lr.fit(train)

# Tahmin
predictions = model.transform(test)

# DeÄŸerlendirme
from pyspark.ml.evaluation import BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator()
auc = evaluator.evaluate(predictions)
print(f"AUC: {auc}")
```

### MLlib AlgoritmalarÄ±
- **Classification:** Logistic Regression, Decision Trees, Random Forest, Naive Bayes
- **Regression:** Linear Regression, Generalized Linear Models
- **Clustering:** K-Means, Gaussian Mixture
- **Collaborative Filtering:** ALS

## ğŸ“Š Spark Streaming

GerÃ§ek zamanlÄ± veri iÅŸleme.

```python
from pyspark.streaming import StreamingContext

# StreamingContext oluÅŸtur (1 saniye batch)
ssc = StreamingContext(spark.sparkContext, 1)

# Veri akÄ±ÅŸÄ±
lines = ssc.socketTextStream("localhost", 9999)

# Transformations
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda a, b: a + b)

# Ã‡Ä±ktÄ±
word_counts.pprint()

# BaÅŸlat
ssc.start()
ssc.awaitTermination()
```

## ğŸ”§ Spark Deployment

### Local Mode
```python
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("MyApp") \
    .getOrCreate()
```

### Cluster Mode
- **Standalone:** Spark'Ä±n kendi cluster manager'Ä±
- **YARN:** Hadoop YARN Ã¼zerinde
- **Mesos:** Apache Mesos Ã¼zerinde
- **Kubernetes:** K8s Ã¼zerinde

## ğŸ’¡ Best Practices

1. **Partitioning:** Veriyi uygun ÅŸekilde bÃ¶lÃ¼mle
2. **Caching:** SÄ±k kullanÄ±lan verileri cache'le
3. **Broadcast variables:** KÃ¼Ã§Ã¼k lookup table'larÄ± broadcast et
4. **Avoid shuffling:** Shuffle iÅŸlemlerini minimize et
5. **Use DataFrames:** RDD yerine DataFrame tercih et
6. **Memory tuning:** Executor/driver memory'yi optimize et

## ğŸ¯ Ne Zaman BÃ¼yÃ¼k Veri AraÃ§larÄ±?

**Kullan:**
- Veri > RAM
- DaÄŸÄ±tÄ±k iÅŸlem gerekli
- Ã–lÃ§eklenebilirlik Ã¶nemli
- Streaming veri

**Kullanma:**
- Veri < 10 GB
- Tek makine yeterli
- Pandas/NumPy ile hallediliyor

## ğŸ“š Kaynaklar

- Apache Spark: https://spark.apache.org/
- PySpark Documentation: https://spark.apache.org/docs/latest/api/python/
- Databricks: https://databricks.com/ (Spark platformu)
- Kaggle Spark Tutorials

## ğŸ’» Kurulum

```bash
# PySpark kurulumu
pip install pyspark

# Jupyter'da kullanÄ±m
pip install jupyter
jupyter notebook
```

```python
# Notebook'ta Spark baÅŸlatma
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
```
