# Hafta 4: Karar AÄŸaÃ§larÄ± (Decision Trees)

## ğŸ“‹ Hafta Hedefleri
- SÄ±nÄ±flandÄ±rma kavramÄ±nÄ± anlama
- Karar aÄŸaÃ§larÄ± algoritmasÄ±nÄ± Ã¶ÄŸrenme
- ID3, C4.5 ve CART algoritmalarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rma
- Bilgi kazancÄ± ve Gini indeksi hesaplama
- Scikit-learn ile karar aÄŸacÄ± modeli oluÅŸturma

## ğŸ“š Teorik Ä°Ã§erik

### 1. SÄ±nÄ±flandÄ±rmaya GiriÅŸ

#### SÄ±nÄ±flandÄ±rma Nedir?
- Denetimli Ã¶ÄŸrenme (Supervised Learning) gÃ¶revidir
- Etiketli veriyle model eÄŸitme
- Yeni verileri kategorilere atama

#### SÄ±nÄ±flandÄ±rma vs Regresyon
- **SÄ±nÄ±flandÄ±rma:** Kategorik hedef deÄŸiÅŸken (spam/ham, hasta/saÄŸlÄ±klÄ±)
- **Regresyon:** SÃ¼rekli hedef deÄŸiÅŸken (fiyat, sÄ±caklÄ±k)

#### SÄ±nÄ±flandÄ±rma SÃ¼reci
1. **EÄŸitim (Training):** Model oluÅŸturma
2. **Test (Testing):** Model deÄŸerlendirme
3. **Tahmin (Prediction):** Yeni verileri sÄ±nÄ±flandÄ±rma

### 2. Karar AÄŸaÃ§larÄ± Nedir?

#### Temel YapÄ±
```
            [Hava Durumu?]
           /      |      \
      GÃ¼neÅŸli   YaÄŸmur   Bulutlu
        /         |         \
   [Nem > 70?]  [HayÄ±r]  [Evet]
    /    \
  HayÄ±r  Evet
   |      |
[Evet] [HayÄ±r]
```

#### BileÅŸenler
- **KÃ¶k DÃ¼ÄŸÃ¼m (Root Node):** En Ã¼stteki dÃ¼ÄŸÃ¼m
- **Ä°Ã§ DÃ¼ÄŸÃ¼mler (Internal Nodes):** Karar noktalarÄ±
- **Yaprak DÃ¼ÄŸÃ¼mler (Leaf Nodes):** SÄ±nÄ±f etiketleri
- **Dal (Branch):** Karar yolu

#### Avantajlar âœ…
- AnlaÅŸÄ±lmasÄ± kolay (white box model)
- GÃ¶rselleÅŸtirilebilir
- Hem sayÄ±sal hem kategorik veri ile Ã§alÄ±ÅŸÄ±r
- Veri Ã¶n iÅŸleme minimal
- DoÄŸrusal olmayan iliÅŸkileri yakalayabilir

#### Dezavantajlar âŒ
- AÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) eÄŸilimi
- KÃ¼Ã§Ã¼k veri deÄŸiÅŸikliklerine duyarlÄ±
- Dengesiz sÄ±nÄ±flarda performans dÃ¼ÅŸer
- XOR gibi problemlerde zayÄ±f

### 3. Karar AÄŸacÄ± OluÅŸturma (Induction)

#### Genel Algoritma
```
BuildTree(Data, Attributes):
    if all same class:
        return leaf with that class
    if no more attributes:
        return leaf with majority class
    
    best_attr = SelectBestAttribute(Data, Attributes)
    tree = new node with best_attr
    
    for each value v of best_attr:
        subset = data where best_attr = v
        subtree = BuildTree(subset, Attributes - {best_attr})
        add subtree to tree
    
    return tree
```

#### Ã–znitelik SeÃ§imi
En iyi Ã¶zniteliÄŸi seÃ§mek iÃ§in metrikler:
- **Bilgi KazancÄ± (Information Gain):** ID3
- **KazanÃ§ OranÄ± (Gain Ratio):** C4.5
- **Gini Ä°ndeksi (Gini Index):** CART

### 4. ID3 AlgoritmasÄ±

#### Entropi (Entropy)
Veri setindeki belirsizlik Ã¶lÃ§Ã¼sÃ¼:
```
Entropy(S) = -Î£ p_i Ã— logâ‚‚(p_i)
```

**Ã–rnek:**
- 9 pozitif, 5 negatif Ã¶rnek
- Entropy = -(9/14)Ã—logâ‚‚(9/14) - (5/14)Ã—logâ‚‚(5/14) = 0.940

#### Bilgi KazancÄ± (Information Gain)
```
Gain(S, A) = Entropy(S) - Î£ (|S_v|/|S|) Ã— Entropy(S_v)
```

**AdÄ±mlar:**
1. TÃ¼m Ã¶znitelikler iÃ§in bilgi kazancÄ±nÄ± hesapla
2. En yÃ¼ksek kazanÃ§lÄ± Ã¶zniteliÄŸi seÃ§
3. O Ã¶zniteliÄŸe gÃ¶re bÃ¶l
4. Alt kÃ¼meler iÃ§in tekrarla

**Ã–rnek Hesaplama:**

Dataset:
```
Day  Outlook  Temp   Humidity  Wind   PlayTennis
D1   Sunny    Hot    High      Weak   No
D2   Sunny    Hot    High      Strong No
D3   Overcast Hot    High      Weak   Yes
D4   Rain     Mild   High      Weak   Yes
D5   Rain     Cool   Normal    Weak   Yes
...
```

1. Entropy(S) = -9/14 Ã— logâ‚‚(9/14) - 5/14 Ã— logâ‚‚(5/14) = 0.940

2. Outlook Ã¶zniteliÄŸi iÃ§in:
   - Sunny: 2 Yes, 3 No â†’ Entropy = 0.971
   - Overcast: 4 Yes, 0 No â†’ Entropy = 0
   - Rain: 3 Yes, 2 No â†’ Entropy = 0.971
   
   Gain(S, Outlook) = 0.940 - (5/14Ã—0.971 + 4/14Ã—0 + 5/14Ã—0.971) = 0.246

#### ID3'Ã¼n ZayÄ±f YÃ¶nleri
- Ã‡ok deÄŸerli Ã¶znitelikleri tercih eder
- SayÄ±sal Ã¶zniteliklerle doÄŸrudan Ã§alÄ±ÅŸmaz
- Eksik deÄŸerleri yÃ¶netemez
- Budama (pruning) yapmaz â†’ aÅŸÄ±rÄ± Ã¶ÄŸrenme

### 5. C4.5 AlgoritmasÄ±

ID3'Ã¼n geliÅŸtirilmiÅŸ hali:

#### KazanÃ§ OranÄ± (Gain Ratio)
```
GainRatio(S, A) = Gain(S, A) / SplitInfo(S, A)

SplitInfo(S, A) = -Î£ (|S_v|/|S|) Ã— logâ‚‚(|S_v|/|S|)
```

- Ã‡ok deÄŸerli Ã¶znitelikleri cezalandÄ±rÄ±r
- Daha dengeli bÃ¶lme yapar

#### C4.5'un Ä°yileÅŸtirmeleri
- âœ… SayÄ±sal Ã¶zniteliklerle Ã§alÄ±ÅŸÄ±r (threshold belirleme)
- âœ… Eksik deÄŸerleri yÃ¶netir
- âœ… Budama (pruning) yapar
- âœ… KazanÃ§ oranÄ± kullanÄ±r

### 6. CART (Classification and Regression Trees)

#### Gini Ä°ndeksi
```
Gini(S) = 1 - Î£ p_iÂ²
```

**Ã–rnek:**
- 9 pozitif, 5 negatif
- Gini = 1 - [(9/14)Â² + (5/14)Â²] = 0.459

#### Gini Split
```
GiniSplit(S, A) = Î£ (|S_v|/|S|) Ã— Gini(S_v)
```

En dÃ¼ÅŸÃ¼k Gini Split deÄŸerine sahip Ã¶zniteliÄŸi seÃ§

#### CART Ã–zellikleri
- Binary splits (ikili bÃ¶lme)
- Hem sÄ±nÄ±flandÄ±rma hem regresyon
- SayÄ±sal ve kategorik verilerle Ã§alÄ±ÅŸÄ±r
- Cost-complexity pruning

### 7. Budama (Pruning)

#### Neden Budama?
- AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi Ã¶nler
- Genelleme yeteneÄŸini artÄ±rÄ±r
- Modeli basitleÅŸtirir

#### Budama TÃ¼rleri

**Pre-pruning (Erken Durdurma)**
- AÄŸaÃ§ bÃ¼yÃ¼rken bazÄ± dallarÄ± bÃ¼yÃ¼tmeme
- Kriterler:
  - Maksimum derinlik
  - Minimum Ã¶rnek sayÄ±sÄ±
  - Minimum bilgi kazancÄ±

**Post-pruning (Sonradan Budama)**
- AÄŸacÄ± tam bÃ¼yÃ¼t
- Sonra gereksiz dallarÄ± kes
- Daha etkili ama yavaÅŸ

#### Cost-Complexity Pruning
```
Cost(T) = Error(T) + Î± Ã— |Leaves(T)|
```
- Î±: Complexity parameter
- Hata ve karmaÅŸÄ±klÄ±k dengesi

### 8. Model DeÄŸerlendirme

#### KarmaÅŸÄ±klÄ±k Matrisi (Confusion Matrix)
```
                Predicted
              Pos    Neg
Actual  Pos   TP     FN
        Neg   FP     TN
```

#### Metrikler
```
Accuracy = (TP + TN) / Total

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

F1-Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```

## ğŸ’» Uygulama Ä°Ã§eriÄŸi

### Lab 1: Scikit-learn ile Karar AÄŸacÄ±
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Model oluÅŸturma
clf = DecisionTreeClassifier(criterion='gini', max_depth=5)
clf.fit(X_train, y_train)

# Tahmin
y_pred = clf.predict(X_test)

# DeÄŸerlendirme
print(accuracy_score(y_test, y_pred))
```

### Lab 2: Karar AÄŸacÄ±nÄ± GÃ¶rselleÅŸtirme
```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20,10))
plot_tree(clf, filled=True, feature_names=features)
plt.show()
```

## ğŸ“ Ã–devler

### Ã–dev 1: Elle Karar AÄŸacÄ± OluÅŸturma
KÃ¼Ã§Ã¼k veri setinde manuel bilgi kazancÄ± hesaplama

### Ã–dev 2: Scikit-learn UygulamasÄ±
Iris veri setinde karar aÄŸacÄ± modeli

## ğŸ”‘ Anahtar Kavramlar
- Supervised learning
- Decision tree induction
- Entropy and information gain
- Gini index
- ID3, C4.5, CART
- Overfitting and pruning
- Confusion matrix
- Model evaluation metrics

## ğŸ’¡ Pratik Ä°puÃ§larÄ±

1. **Max depth ayarlayÄ±n:**
   ```python
   clf = DecisionTreeClassifier(max_depth=5)
   ```

2. **Min samples split:**
   ```python
   clf = DecisionTreeClassifier(min_samples_split=10)
   ```

3. **Feature importance:**
   ```python
   importances = clf.feature_importances_
   ```

4. **Cross-validation kullanÄ±n:**
   ```python
   from sklearn.model_selection import cross_val_score
   scores = cross_val_score(clf, X, y, cv=5)
   ```

## â“ SÄ±kÃ§a Sorulan Sorular

**S: Gini mi Entropy mi kullanmalÄ±yÄ±m?**
C: Genelde Ã§ok fark etmez. Gini biraz daha hÄ±zlÄ± hesaplanÄ±r.

**S: Max depth ne kadar olmalÄ±?**
C: Cross-validation ile bulun. Genelde 3-10 arasÄ± iyi sonuÃ§ verir.

**S: Karar aÄŸacÄ± regresyon iÃ§in kullanÄ±labilir mi?**
C: Evet, CART hem sÄ±nÄ±flandÄ±rma hem regresyon yapabilir.

---

**Sonraki Hafta:** Kural Ã‡Ä±karÄ±mÄ± ve k-NN AlgoritmasÄ±
