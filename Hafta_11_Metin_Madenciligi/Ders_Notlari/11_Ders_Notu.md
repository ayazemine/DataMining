# Hafta 11: Metin MadenciliÄŸi ve NLP

## ğŸ“‹ Metin MadenciliÄŸi Nedir?

YapÄ±sal olmayan metinlerden bilgi Ã§Ä±karma sÃ¼reci.

### Uygulama AlanlarÄ±
- Duygu analizi (Sentiment Analysis)
- Spam filtreleme
- Chatbot'lar
- Makine Ã§evirisi
- Metin Ã¶zetleme
- Soru-cevap sistemleri

## ğŸ”§ Metin Ã–n Ä°ÅŸleme

### 1. Tokenization (Simgeleme)
```python
text = "Veri madenciliÄŸi Ã§ok ilginÃ§!"
tokens = text.split()  # ['Veri', 'madenciliÄŸi', 'Ã§ok', 'ilginÃ§!']
```

### 2. Lowercasing (KÃ¼Ã§Ã¼k harfe Ã§evirme)
```python
text = text.lower()  # "veri madenciliÄŸi Ã§ok ilginÃ§!"
```

### 3. Noktalama Ä°ÅŸaretlerini KaldÄ±rma
```python
import string
text = text.translate(str.maketrans('', '', string.punctuation))
```

### 4. Stop Words (Durak Kelimeleri) KaldÄ±rma
```python
from nltk.corpus import stopwords
stop_words = set(stopwords.words('turkish'))
tokens = [w for w in tokens if w not in stop_words]
```

### 5. Stemming (KÃ¶k Bulma)
```python
from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer('turkish')
stemmed = [stemmer.stem(word) for word in tokens]
```

### 6. Lemmatization (KÃ¶k SÃ¶zcÃ¼k Bulma)
```python
import spacy
nlp = spacy.load('tr_core_news_sm')
doc = nlp(text)
lemmas = [token.lemma_ for token in doc]
```

## ğŸ“Š Metin Temsili

### Bag of Words (BoW)
```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'Veri madenciliÄŸi Ã¶ÄŸreniyorum',
    'Python ile veri analizi',
    'Metin madenciliÄŸi projesi'
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
```

### TF-IDF (Term Frequency-Inverse Document Frequency)

**TF:** Kelime sÄ±klÄ±ÄŸÄ±
```
TF(t, d) = (DokÃ¼mandaki t sayÄ±sÄ±) / (DokÃ¼mandaki toplam kelime)
```

**IDF:** Ters dokÃ¼man frekansÄ±
```
IDF(t) = log(Toplam dokÃ¼man / t iÃ§eren dokÃ¼man)
```

**TF-IDF:**
```
TF-IDF(t, d) = TF(t, d) Ã— IDF(t)
```

```python
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=1000)
X = tfidf.fit_transform(corpus)
```

## ğŸ˜Š Duygu Analizi (Sentiment Analysis)

### YaklaÅŸÄ±mlar

**1. SÃ¶zlÃ¼k TabanlÄ± (Lexicon-Based)**
```python
positive_words = ['iyi', 'gÃ¼zel', 'harika', 'mÃ¼kemmel']
negative_words = ['kÃ¶tÃ¼', 'berbat', 'ì•ˆì¢‹ì€']

def sentiment_score(text):
    words = text.lower().split()
    pos = sum(1 for w in words if w in positive_words)
    neg = sum(1 for w in words if w in negative_words)
    return pos - neg
```

**2. Makine Ã–ÄŸrenmesi TabanlÄ±**
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# Veri: (metin, etiket) - etiket: 0=negatif, 1=pozitif
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

nb = MultinomialNB()
nb.fit(X_train, y_train)
predictions = nb.predict(X_test)
```

**3. Derin Ã–ÄŸrenme TabanlÄ±**
- LSTM, GRU
- BERT, GPT
- Transformer modelleri

## ğŸ“– Konu Modelleme (Topic Modeling)

### LDA (Latent Dirichlet Allocation)

Her dokÃ¼man, konularÄ±n bir karÄ±ÅŸÄ±mÄ±dÄ±r.
Her konu, kelimelerin bir daÄŸÄ±lÄ±mÄ±dÄ±r.

```python
from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(X)

# KonularÄ± gÃ¶rÃ¼ntÃ¼le
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda.components_):
    top_words = [feature_names[i] for i in topic.argsort()[-10:]]
    print(f"Konu {topic_idx}: {', '.join(top_words)}")
```

## ğŸŒ Word Embeddings

### Word2Vec
Kelimeleri vektÃ¶r uzayÄ±nda temsil eder.

```python
from gensim.models import Word2Vec

sentences = [['veri', 'madenciliÄŸi'], ['metin', 'analizi']]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# Benzer kelimeler
similar = model.wv.most_similar('veri', topn=5)
```

## ğŸ’» Pratik Ã–rnek: Twitter Duygu Analizi

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Veri yÃ¼kleme
df = pd.read_csv('tweets.csv')

# Ã–n iÅŸleme
def preprocess(text):
    text = text.lower()
    # Daha fazla Ã¶n iÅŸleme...
    return text

df['clean_text'] = df['text'].apply(preprocess)

# TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['clean_text'])
y = df['sentiment']  # 0: negatif, 1: pozitif

# Model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
nb = MultinomialNB()
nb.fit(X_train, y_train)

# DeÄŸerlendirme
y_pred = nb.predict(X_test)
print(classification_report(y_test, y_pred))
```

## ğŸ”‘ Ã–nemli KÃ¼tÃ¼phaneler

- **NLTK:** Natural Language Toolkit
- **spaCy:** HÄ±zlÄ± NLP kÃ¼tÃ¼phanesi
- **Gensim:** Topic modeling ve word embeddings
- **TextBlob:** Basit sentiment analysis
- **Transformers (Hugging Face):** BERT, GPT modelleri

## ğŸ“š TÃ¼rkÃ§e NLP KaynaklarÄ±

```python
# NLTK TÃ¼rkÃ§e
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
turkish_stops = stopwords.words('turkish')

# spaCy TÃ¼rkÃ§e
# python -m spacy download tr_core_news_sm
import spacy
nlp = spacy.load('tr_core_news_sm')
```

## ğŸ¯ Ä°puÃ§larÄ±

1. **Veri temizliÄŸi Ã¶nemli:** Metin verisi Ã§ok kirli olabilir
2. **Domain knowledge:** Sosyal medya vs haber metni farklÄ±
3. **Class imbalance:** Duygu analizinde sÄ±k gÃ¶rÃ¼lÃ¼r
4. **Preprocessing pipeline:** TutarlÄ± Ã¶n iÅŸleme
5. **Model seÃ§imi:** BaÅŸla basit (Naive Bayes), gerekirse karmaÅŸÄ±klaÅŸtÄ±r
