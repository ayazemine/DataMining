# Hafta 9: KÃ¼meleme AlgoritmalarÄ±

## ğŸ“‹ KÃ¼meleme Nedir?

**Unsupervised learning** - Etiket yok, benzer verileri gruplama

### KullanÄ±m AlanlarÄ±
- MÃ¼ÅŸteri segmentasyonu
- GÃ¶rÃ¼ntÃ¼ segmentasyonu
- Anomali tespiti
- Veri sÄ±kÄ±ÅŸtÄ±rma
- Ã–n iÅŸleme

## ğŸ¯ K-Means AlgoritmasÄ±

### Algoritma AdÄ±mlarÄ±
1. K merkez noktasÄ± rastgele seÃ§
2. Her veriyi en yakÄ±n merkeze ata
3. Merkezleri gÃ¼ncelle (kÃ¼menin ortalamasÄ±)
4. DeÄŸiÅŸim olmazsa dur, yoksa 2'ye dÃ¶n

### K-Means++
BaÅŸlangÄ±Ã§ merkezlerini akÄ±llÄ±ca seÃ§erek daha iyi sonuÃ§.

### K DeÄŸeri SeÃ§imi

**Elbow Method:**
```python
inertias = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)
plt.plot(range(1, 11), inertias)
```

**Silhouette Score:**
```
s = (b - a) / max(a, b)
s âˆˆ [-1, 1], yÃ¼ksek iyi
```

### Avantajlar & Dezavantajlar

âœ… Basit ve hÄ±zlÄ±
âœ… BÃ¼yÃ¼k veri setlerinde etkili
âœ… Ã–lÃ§eklenebilir

âŒ K deÄŸeri Ã¶nceden bilinmeli
âŒ AykÄ±rÄ± deÄŸerlere duyarlÄ±
âŒ KÃ¼resel olmayan kÃ¼meler iÃ§in zayÄ±f
âŒ FarklÄ± boyut/yoÄŸunlukta kÃ¼meler iÃ§in uygun deÄŸil

## ğŸŒ³ HiyerarÅŸik KÃ¼meleme

### Agglomerative (Bottom-Up)
1. Her veri ayrÄ± kÃ¼me
2. En yakÄ±n iki kÃ¼meyi birleÅŸtir
3. Tek kÃ¼me kalana kadar tekrarla

### Divisive (Top-Down)
1. TÃ¼m veri tek kÃ¼me
2. En heterojen kÃ¼meyi bÃ¶l
3. Her veri ayrÄ± kÃ¼me olana kadar

### Linkage MetodlarÄ±
- **Single:** En yakÄ±n noktalar arasÄ± mesafe
- **Complete:** En uzak noktalar arasÄ± mesafe
- **Average:** Ortalama mesafe
- **Ward:** Varyans minimizasyonu (en popÃ¼ler)

### Dendrogram
```python
from scipy.cluster.hierarchy import dendrogram, linkage

Z = linkage(X, method='ward')
dendrogram(Z)
plt.show()
```

## ğŸ¨ DBSCAN (Density-Based)

### Parametreler
- **eps (Îµ):** KomÅŸuluk yarÄ±Ã§apÄ±
- **min_samples:** Minimum nokta sayÄ±sÄ±

### Nokta Tipleri
- **Core point:** eps iÃ§inde min_samples kadar komÅŸu var
- **Border point:** Core point komÅŸusu ama kendisi core deÄŸil
- **Noise:** Ne core ne border (outlier)

### Avantajlar
âœ… K deÄŸeri gerekmez
âœ… Herhangi ÅŸekilde kÃ¼me bulabilir
âœ… Noise/outlier tespit eder
âœ… FarklÄ± boyutlarda kÃ¼meler

âŒ eps ve min_samples seÃ§imi zor
âŒ FarklÄ± yoÄŸunlukta kÃ¼meler iÃ§in zayÄ±f

## ğŸ“Š KÃ¼me DeÄŸerlendirme

### Ä°Ã§ Metrikler (Internal)
- **Silhouette Score:** [-1, 1], yÃ¼ksek iyi
- **Davies-Bouldin Index:** DÃ¼ÅŸÃ¼k iyi
- **Calinski-Harabasz Index:** YÃ¼ksek iyi

### DÄ±ÅŸ Metrikler (External) - Etiket varsa
- **Adjusted Rand Index (ARI)**
- **Normalized Mutual Information (NMI)**

## ğŸ’» Python Kodu

```python
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# HiyerarÅŸik
hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')
labels = hierarchical.fit_predict(X)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# DeÄŸerlendirme
score = silhouette_score(X, labels)
print(f"Silhouette Score: {score:.3f}")
```

## ğŸ¯ Hangi AlgoritmayÄ± SeÃ§meli?

- **K-Means:** HÄ±zlÄ±, kÃ¼resel kÃ¼meler, K biliniyor
- **HiyerarÅŸik:** K bilinmiyor, kÃ¼Ã§Ã¼k veri, dendrogram gerekli
- **DBSCAN:** FarklÄ± ÅŸekiller, outlier Ã¶nemli, yoÄŸunluk tabanlÄ±

## ğŸ“ˆ GÃ¶rselleÅŸtirme

```python
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], 
            kmeans.cluster_centers_[:, 1], 
            marker='x', s=200, c='red', label='Merkezler')
plt.legend()
plt.show()
```
