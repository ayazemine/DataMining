# Hafta 12: Web MadenciliÄŸi ve Tavsiye Sistemleri

## ðŸ“‹ Web MadenciliÄŸi Nedir?

Web'den otomatik olarak yararlÄ± bilgi Ã§Ä±karma.

## ðŸŒ Web MadenciliÄŸi TÃ¼rleri

### 1. Web Content Mining (Ä°Ã§erik MadenciliÄŸi)
- Metin, resim, video analizi
- Arama motorlarÄ±
- Duygu analizi

### 2. Web Structure Mining (YapÄ± MadenciliÄŸi)
- Link analizi
- PageRank
- HITS algoritmasÄ±

### 3. Web Usage Mining (KullanÄ±m MadenciliÄŸi)
- KullanÄ±cÄ± davranÄ±ÅŸ analizi
- Clickstream analizi
- Session analizi

## ðŸ”— PageRank AlgoritmasÄ±

### Temel Ä°dea
"Ã–nemli sayfalardan gelen linkler daha deÄŸerlidir."

### FormÃ¼l
```
PR(A) = (1-d) + d Ã— Î£(PR(Ti) / C(Ti))

PR(A): A sayfasÄ±nÄ±n PageRank'i
d: Damping factor (0.85)
Ti: A'ya link veren sayfalar
C(Ti): Ti'nin dÄ±ÅŸa verdiÄŸi link sayÄ±sÄ±
```

### Python Implementasyonu

```python
import numpy as np

def pagerank(M, num_iterations=100, d=0.85):
    """
    M: Link matrisi (MxM), M[i][j] = 1 if i->j
    """
    N = M.shape[1]
    v = np.random.rand(N, 1)
    v = v / np.linalg.norm(v, 1)
    
    M_hat = (d * M + (1 - d) / N)
    
    for i in range(num_iterations):
        v = M_hat @ v
    
    return v

# Ã–rnek
M = np.array([
    [0, 1/2, 1/2, 0],
    [1/3, 0, 0, 1/2],
    [1/3, 0, 0, 1/2],
    [1/3, 1/2, 1/2, 0]
])

ranks = pagerank(M)
print("PageRank skorlarÄ±:", ranks.flatten())
```

### NetworkX ile PageRank

```python
import networkx as nx

G = nx.DiGraph()
G.add_edges_from([(1, 2), (1, 3), (2, 4), (3, 4), (4, 2)])

pr = nx.pagerank(G, alpha=0.85)
print("PageRank:", pr)
```

## ðŸŽ¯ HITS AlgoritmasÄ±

**Hyperlink-Induced Topic Search**

### Ä°ki TÃ¼r Sayfa
- **Authorities:** Ä°yi iÃ§erik sayfalarÄ±
- **Hubs:** Ä°yi authority'lere link veren sayfalar

### FormÃ¼ller
```
Authority Score: a(p) = Î£ h(q)  (p'ye link veren q'larÄ±n hub skorlarÄ±)
Hub Score: h(p) = Î£ a(q)  (p'nin link verdiÄŸi q'larÄ±n authority skorlarÄ±)
```

```python
def hits(G, max_iter=100):
    nodes = list(G.nodes())
    h = {node: 1.0 for node in nodes}
    a = {node: 1.0 for node in nodes}
    
    for _ in range(max_iter):
        # Update authority
        for node in nodes:
            a[node] = sum(h[pred] for pred in G.predecessors(node))
        
        # Update hub
        for node in nodes:
            h[node] = sum(a[succ] for succ in G.successors(node))
        
        # Normalize
        norm_a = sum(a.values())
        norm_h = sum(h.values())
        a = {k: v/norm_a for k, v in a.items()}
        h = {k: v/norm_h for k, v in h.items()}
    
    return h, a

hubs, authorities = hits(G)
```

## ðŸ›’ Tavsiye Sistemleri (Recommender Systems)

### TÃ¼rleri

**1. Content-Based Filtering**
ÃœrÃ¼n Ã¶zelliklerine gÃ¶re tavsiye.
```
KullanÄ±cÄ± aksiyon filmlerini seviyor â†’ Aksiyon filmleri Ã¶ner
```

**2. Collaborative Filtering**
Benzer kullanÄ±cÄ±larÄ±n tercihlerine gÃ¶re tavsiye.
```
User A ve User B benzer â†’ A'nÄ±n beÄŸendiÄŸi, B'nin gÃ¶rmediÄŸini Ã¶ner
```

**3. Hybrid Systems**
Her iki yÃ¶ntemi birleÅŸtir.

## ðŸ‘¥ Collaborative Filtering

### User-Based CF

**AdÄ±mlar:**
1. KullanÄ±cÄ±lar arasÄ± benzerlik hesapla
2. Benzer kullanÄ±cÄ±larÄ± bul
3. OnlarÄ±n beÄŸendiÄŸi Ã¼rÃ¼nleri Ã¶ner

**Cosine Similarity:**
```python
from sklearn.metrics.pairwise import cosine_similarity

# user_item_matrix: kullanÄ±cÄ± x Ã¼rÃ¼n
user_similarity = cosine_similarity(user_item_matrix)

# En benzer 5 kullanÄ±cÄ±
similar_users = user_similarity[user_id].argsort()[-6:-1][::-1]
```

### Item-Based CF

**AdÄ±mlar:**
1. ÃœrÃ¼nler arasÄ± benzerlik hesapla
2. KullanÄ±cÄ±nÄ±n beÄŸendiÄŸi Ã¼rÃ¼nlere benzer Ã¼rÃ¼nler bul
3. Ã–ner

```python
# ÃœrÃ¼n benzerliÄŸi
item_similarity = cosine_similarity(user_item_matrix.T)

# KullanÄ±cÄ±nÄ±n beÄŸendiÄŸi Ã¼rÃ¼nler
liked_items = user_item_matrix[user_id] > 0

# Benzer Ã¼rÃ¼nler
recommendations = item_similarity[liked_items].sum(axis=0)
```

## ðŸ§® Matrix Factorization

### SVD (Singular Value Decomposition)

```
R â‰ˆ U Ã— Î£ Ã— V^T

R: user_item_matrix (m Ã— n)
U: user_features (m Ã— k)
Î£: singular values (k Ã— k)
V: item_features (n Ã— k)
```

### Python ile SVD

```python
from scipy.sparse.linalg import svds

# SVD
U, sigma, Vt = svds(user_item_matrix, k=50)

# Tahmin
sigma = np.diag(sigma)
predictions = np.dot(np.dot(U, sigma), Vt)

# En yÃ¼ksek skorlu Ã¼rÃ¼nler
user_predictions = predictions[user_id]
top_items = user_predictions.argsort()[-10:][::-1]
```

### Surprise KÃ¼tÃ¼phanesi

```python
from surprise import SVD, Dataset, Reader
from surprise.model_selection import cross_validate

# Veri yÃ¼kleme
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)

# SVD modeli
algo = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02)

# Cross-validation
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# EÄŸitim
trainset = data.build_full_trainset()
algo.fit(trainset)

# Tahmin
pred = algo.predict(user_id, movie_id)
print(f"Predicted rating: {pred.est:.2f}")
```

## ðŸ“Š DeÄŸerlendirme Metrikleri

### Rating Prediction

**RMSE (Root Mean Square Error):**
```
RMSE = âˆš(Î£(rÌ‚ - r)Â² / n)
```

**MAE (Mean Absolute Error):**
```
MAE = Î£|rÌ‚ - r| / n
```

### Top-N Recommendation

**Precision@K:**
```
Precision@K = |Ä°lgili âˆ© Ã–nerilen| / K
```

**Recall@K:**
```
Recall@K = |Ä°lgili âˆ© Ã–nerilen| / |Ä°lgili|
```

**MAP (Mean Average Precision)**

**NDCG (Normalized Discounted Cumulative Gain)**

```python
from sklearn.metrics import ndcg_score

# y_true: gerÃ§ek relevance skorlarÄ±
# y_score: tahmin skorlarÄ±
ndcg = ndcg_score([y_true], [y_score], k=10)
```

## ðŸŽ¬ Pratik Ã–rnek: Film Tavsiye Sistemi

```python
import pandas as pd
from surprise import SVD, Dataset, Reader
from surprise.model_selection import train_test_split

# MovieLens veri seti
ratings = pd.read_csv('ratings.csv')

# Surprise formatÄ±na Ã§evir
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

# Train-test split
trainset, testset = train_test_split(data, test_size=0.2)

# Model
algo = SVD(n_factors=100, n_epochs=20)
algo.fit(trainset)

# Test
predictions = algo.test(testset)

# Metriklere
from surprise import accuracy
rmse = accuracy.rmse(predictions)
mae = accuracy.mae(predictions)

# KullanÄ±cÄ±ya tavsiye
def get_top_n_recommendations(algo, user_id, n=10):
    # TÃ¼m filmler
    all_movies = ratings['movieId'].unique()
    
    # KullanÄ±cÄ±nÄ±n izlediÄŸi filmler
    watched = ratings[ratings['userId'] == user_id]['movieId'].values
    
    # Ä°zlemediÄŸi filmler
    not_watched = [m for m in all_movies if m not in watched]
    
    # Tahminler
    predictions = [algo.predict(user_id, movie_id) for movie_id in not_watched]
    
    # SÄ±rala
    predictions.sort(key=lambda x: x.est, reverse=True)
    
    # Top N
    return predictions[:n]

# Ã–rnek kullanÄ±m
user_id = 1
recommendations = get_top_n_recommendations(algo, user_id, n=10)

print(f"Top 10 film tavsiyeleri (User {user_id}):")
for pred in recommendations:
    print(f"MovieID: {pred.iid}, Predicted Rating: {pred.est:.2f}")
```

## ðŸ”¥ Cold Start Problemi

Yeni kullanÄ±cÄ±/Ã¼rÃ¼n iÃ§in yeterli veri yok.

### Ã‡Ã¶zÃ¼mler
1. **Hybrid approach:** Content-based + collaborative
2. **Popularity-based:** PopÃ¼ler Ã¼rÃ¼nleri Ã¶ner
3. **User profiling:** Ä°lk kayÄ±tta tercihler sor
4. **Side information:** Demographic bilgi kullan

## ðŸ’¡ Ä°puÃ§larÄ±

1. **Implicit feedback:** Like, click, view â†’ explicit rating'den daha bol
2. **Temporal dynamics:** KullanÄ±cÄ± tercihleri zamanla deÄŸiÅŸir
3. **Diversity:** Sadece benzer Ã¼rÃ¼n Ã¶nerme
4. **Serendipity:** SÃ¼rpriz Ã¶neriler
5. **Context-aware:** Zaman, lokasyon, cihaz bilgisi kullan
6. **Scalability:** BÃ¼yÃ¼k veri setleri iÃ§in approximate methods

## ðŸ“š KÃ¼tÃ¼phaneler

- **Surprise:** Python iÃ§in collaborative filtering
- **LightFM:** Hybrid recommender systems
- **Implicit:** Implicit feedback iÃ§in CF
- **TensorFlow Recommenders:** Neural collaborative filtering
- **NetworkX:** Graph-based methods (PageRank, HITS)
